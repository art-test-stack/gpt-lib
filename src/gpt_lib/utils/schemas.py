from gpt_lib.utils.import_utils import is_flash_attn3_available_from_kernel

import torch
import torch.distributed as dist
from pydantic import BaseModel, Field, ConfigDict, PrivateAttr
from typing import Any, List, Literal, Optional, get_args
from pathlib import Path
import json
import pickle

from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings

from gpt_lib.utils.default import (
    DEVICE,
    MODELS_FOLDER, 
    VOCAB_SIZE, 
    MAX_CONTEXT, 
    NUM_HEADS, 
    NUM_LAYERS, 
    DIM_MODEL, 
    DIM_FFN, 
    DIM_HEAD, 
    DROPOUT, 
    BATCH_SIZE, 
    MAX_LEARNING_RATE, 
    MIN_LEARNING_RATE, 
    WARMUP_ITERS, 
    VALIDATION_STEP, 
    PRETRAINING_VAL_RATIO,
    PAT_STR_GPT2,
    PAT_STR_GPT4,
    adamw_opt_params,
    opt_params
)
from gpt_lib.utils.types import (
    AttnImplTypes,
    Betas2,
    Devices,
    Dtypes,
    LossReductionTypes,
    LossTypes,
    NormalizationTypes,
    Nus2,
    OptimizerNames,
    PositionalEncodingTypes,
    TfTypes,
    TokenizerSources,
    TokenizerTensors,
    TParams,
    TpModes,
)
from gpt_lib.utils.special_tokens import SpecialTokens


def get_default_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")


class ParallelismConfig(BaseModel):
    model_config = ConfigDict(frozen=True)

    enabled: bool = False

    mode: TpModes = "dp"
    world_size: int
    tp_size: int = 1
    dp_size: int = 1

    tp_size: int = 1
    tp_rank: int = 0
    # dp_group: dist.ProcessGroup | None = None
    # tp_group: dist.ProcessGroup | None = None

    n_heads_q: Optional[int] = None
    n_heads_kv: Optional[int] = None
    d_head_q: Optional[int] = None

    tp_mode: TpModes = "row"

    @property
    def local_heads_q(self) -> int:
        if self.n_heads_q is None:
            raise ValueError("n_heads_q is not set for TensorParallelConfig")
        return self.n_heads_q // self.tp_size
    
    @property
    def local_heads_kv(self) -> int:
        if self.n_heads_kv is None:
            raise ValueError("n_heads_kv is not set for TensorParallelConfig")
        return self.n_heads_kv // self.tp_size

class TokenizerConfig(BaseModel):
    name: str = "ic1_tok"
    dirname: Path = MODELS_FOLDER
    vocab_size: int = VOCAB_SIZE
    max_context: Optional[int] = MAX_CONTEXT
    pat_str: Optional[str] = PAT_STR_GPT2
    special_tokens: Optional[SpecialTokens] = Field(default_factory=SpecialTokens)
    source: TokenizerSources = "tiktoken"

    def model_post_init(self, context: Any) -> None:
        self.dirname = self.dirname / self.name
        if not self.dirname.exists():
            self.dirname.mkdir(parents=True, exist_ok=True)

    def get_mergeable_ranks(self) -> dict:
        if not self.dirname.exists():
            raise FileNotFoundError(f"Tokenizer directory {self.dirname} does not exist.")
        mergeable_ranks_path = self.dirname / "mergeable_ranks.pkl"
        if not mergeable_ranks_path.exists():
            raise FileNotFoundError(f"Mergeable ranks file {mergeable_ranks_path} does not exist.")
        with open(mergeable_ranks_path, "rb") as f:
            mergeable_ranks = pickle.load(f)
        assert len(mergeable_ranks) == self.vocab_size, "Mergeable ranks size does not match vocab size."
        return mergeable_ranks

class TrainingTokenizerConfig(TokenizerConfig):
    max_chars: int = 10_000_000_000
    chars_per_doc: int = 10_000
    merges_per_pass: int = 512

class TransformerConfig(BaseModel):
    # model_config = ConfigDict(frozen=True)
    tf_type: TfTypes = "dense"

    vocab_size: int = VOCAB_SIZE
    max_context: int = MAX_CONTEXT
    pad_id: int = -100
    bos_id: int = -100
    eos_id: int = -100

    positional_encoding: PositionalEncodingTypes = "rope" # Options: "positional", "rope"

    # TODO: Same structure as transformers.RopeParameters for huggingface compatibility
    # https://huggingface.co/docs/transformers/v5.0.0rc1/internal/rope_utils
    rope_params: dict = Field(default_factory=lambda: {"rope_theta": 10_000, "rope_type": "default"})  # Used if positional_encoding is "rope"

    d_model: int = DIM_MODEL
    d_ffn: int = DIM_FFN  # 4 * dim_model
    n_heads: int = NUM_HEADS
    n_kv_heads: Optional[int] = None # GQA
    n_layers: int = NUM_LAYERS
    d_head: int = DIM_HEAD  # dim_model // num_heads
    tie_word_embeddings: bool = True # TODO: implement it in model

    dropout: float = DROPOUT
    attention_dropout: Optional[float] = None

    norm_before_attn: bool = True
    normalization: NormalizationTypes = "rms"  # Options: "rms", "layer"
    norm_eps: float = 1e-8
    act_func: str = "gelu" # TODO: make it compatible with model.Transformer implementation

    # TODO: padged attention implementation
    attn_impl: AttnImplTypes = "sdpa"  # Options: "sdpa", "flash_attention", "impl". Not recommended : "impl" if return_weights=False.
    # TODO: # layer_types: Optional[List[TParams]] = None  # e.g., ["standard", "standard", "moe", ...] length must be n_layers
    enable_gqa: bool = False

    # Sliding window attention pattern string, tiled across layers. Final layer always L.
    # Characters: L=long (full context), S=short (half context)
    # Examples: "L"=all full context, "SL"=alternating, "SSL"=two short then one long
    # Based on: https://github.com/karpathy/nanochat/blob/master/nanochat/gpt.py
    window_pattern: str = "SSSL" # Can only be composed of 'L' and 'S' characters
    window_size: Optional[int] = None  # Size of short windows
    max_window_size: Optional[int] = None  # Maximum size of short windows (for dynamic window sizing)
    _window_sizes: List[tuple[int, int]] = PrivateAttr(default_factory=list) # TODO later: make it dynamic

    softcap: float = 18.0
    
    def model_post_init(self, context: Any) -> None:
        if self.d_model % self.n_heads != 0:
            raise ValueError(f"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})")
        if self.d_head != self.d_model // self.n_heads:
            warnings.warn(f"d_head ({self.d_head}) is not equal to d_model/n_heads ({self.d_model // self.n_heads}). This may lead to unexpected behavior in attention mechanisms.")
        
        self.n_kv_heads = getattr(self, "n_kv_heads", None) or self.n_heads
        self.attention_dropout = self.attention_dropout if self.attention_dropout is not None else self.dropout

        if not self.norm_before_attn:
            warnings.warn("Using post-attention normalization (norm_before_attn=False) may lead to training instability.")
        
        # TODO: handles warnings fallbacks
        # if self.attn_impl == "flash_attention":
        #     if not is_flash_attn3_available_from_kernel():
        #         warnings.warn("FlashAttention 3 kernel is not available. Falling back to standard attention.")
        #         self.attn_impl = "sdpa"
        #     # try:
        #     #     import flash_attn
        #     # except ImportError:
        #     #     warnings.warn("FlashAttention is not installed. Falling back to standard attention.")
        #     #     self.attn_impl = "sdpa"
        # if self.attn_impl == "impl":
        #     warnings.warn("Using 'impl' attention type is not recommended for production use. Only use for experimentation or retrieve attention weights.")

        self._window_sizes = self._compute_window()
        # freeze model_config manually to prevent issues with nested models
        self.model_config["frozen"] = True
        

    def _compute_window(self) -> str:
        pattern = self.window_pattern.upper()
        assert all(c in {'L', 'S'} for c in pattern), "Invalid characters in window_pattern. Only 'L' and 'S' are allowed."

        short_window_size = self.window_size or (self.max_context // 2)
        window_table = {
            'L': (-1, 0), # or (self.max_context, 0) works
            'S': (short_window_size, 0)
        }
        window_sizes = []
        for idx in range(self.n_layers - 1):
            char = pattern[idx % len(pattern)]
            window_sizes.append(window_table[char])
        window_sizes.append((-1, 0))  # Final layer always long
        return window_sizes
    
class DenseTransformerConfig(TransformerConfig):
    pass

class MoETransformerConfig(TransformerConfig):
    tf_type: TfTypes = "moe"
    nb_experts: int = 16
    expert_capacity_factor: float = 1.0

class LossConfig(BaseModel):
    loss_fn: LossTypes = "cross_entropy"
    kwargs: dict = Field(default_factory=dict)
    ignore_index: int = -100
    reduction: LossReductionTypes = "none"

class GenerationConfig(BaseModel):
    max_length: int = 256
    temperature: float = 1.0
    top_k: int = 0
    top_p: float = 1.0
    repetition_penalty: float = 1.0
    do_sample: bool = True
    num_return_sequences: int = 1
    seed: Optional[int] = None
    stream: bool = False
    use_cache: bool = True

    def model_post_init(self, context: Any) -> None:
        if self.max_length <= 0:
            raise ValueError("max_length must be a positive integer.")
        if self.temperature <= 0.0:
            raise ValueError("temperature must be a positive float.")
        if not (0.0 <= self.top_p <= 1.0):
            raise ValueError("top_p must be in the range [0.0, 1.0].")
        if self.num_return_sequences <= 0:
            raise ValueError("num_return_sequences must be a positive integer.")
        if self.seed is None or self.seed < 0:
            self.seed = 42  # Ensure seed is within valid range for torch.manual_seed

class OptimizerSpec(BaseModel):
    name: OptimizerNames = "adamw"
    kwargs: dict = Field(default_factory=dict)

    def optimizer_class(self) -> Any:
        # TODO: change it to include custom optimizers
        import gpt_lib.optimizers.optim as optim_module
        opt_class = optim_module.opt_class.get(self.name, None)
        if opt_class is not None:
            return opt_class
        if self.name not in optim_module.opt_class:
            raise ValueError(f"Optimizer '{self.name}' is not recognized. Available optimizers: {list(optim_module.opt_class.keys())}")

class TrainingConfig(BaseModel):
    model_config = ConfigDict(frozen=True)
    batch_size: int = BATCH_SIZE
    steps: int = 100000
    accumulation_steps: int = 100

    batch_size_scheduling: bool = False
    max_learning_rate: float = MAX_LEARNING_RATE
    min_learning_rate: float = MIN_LEARNING_RATE
    warmup_iters: int = WARMUP_ITERS

    optimizer: dict[str, OptimizerSpec] | OptimizerSpec | OptimizerNames | None = None # { "emb": Optimizer(...), "tf": Optimizer(...) } or single Optimizer or "opt_name" 
    _optimizers: dict[str, OptimizerSpec] = PrivateAttr(default_factory=dict)

    validation_step: int = VALIDATION_STEP
    pretraining_val_ratio: float = PRETRAINING_VAL_RATIO

    def model_post_init(self, context: Any) -> None:
        # TODO: TODO: 0-indexed layer number
        opt: dict[str, OptimizerSpec] = dict()

        if isinstance(self.optimizer, str):
            if self.optimizer not in get_args(OptimizerNames):
                raise ValueError(f"optimizer string must be one of {OptimizerNames}. Got {self.optimizer}.")
            warnings.warn(f"Using a single optimizer for both embeddings and transformer layers. {self.optimizer} is used with default parameters: {opt_params[self.optimizer]}.")
            self._optimizers = {
                "emb": OptimizerSpec(name=self.optimizer, kwargs=opt_params[self.optimizer]),
                "tf": OptimizerSpec(name=self.optimizer, kwargs=opt_params[self.optimizer]),
                "lm_head": OptimizerSpec(name=self.optimizer, kwargs=opt_params[self.optimizer]),
                "w_x0": OptimizerSpec(name=self.optimizer, kwargs=opt_params[self.optimizer]),
                "w_res": OptimizerSpec(name=self.optimizer, kwargs=opt_params[self.optimizer]),
            }
        elif isinstance(self.optimizer, dict):
            for key, value in self.optimizer.items():
                if key not in {"emb", "tf", "lm_head", "w_x0", "w_res"}: # emb, tf, lm_head, w_x0, w_res layers
                    raise ValueError(f"optimizer dict keys must be 'emb' and/or 'tf'. Got {key}.")
                if isinstance(value, OptimizerSpec):
                    opt[key] = value
                elif isinstance(value, str):
                    opt[key] = OptimizerSpec(name=value, kwargs=opt_params[value])
                else:
                    raise ValueError(f"optimizer[{key}] must be str or OptimizerSpec. Got {type(value)}.")

        elif isinstance(self.optimizer, OptimizerSpec):
            warnings.warn(f"Using a single optimizer for both embeddings and transformer layers. {self.optimizer.name} is used with specified parameters: {self.optimizer.kwargs}.")
            opt["emb"] = self.optimizer
            opt["tf"] = self.optimizer
            opt["head"] = self.optimizer
            opt["w_x0"] = self.optimizer
            opt["w_res"] = self.optimizer
        else:
            warnings.warn("No optimizer specified. Using default optimizers: AdamW for embeddings and Muon for transformer layers.")
        
        opt.setdefault("emb", OptimizerSpec(name="adamw", kwargs=opt_params["adamw"]))
        opt.setdefault("tf", OptimizerSpec(name="muon", kwargs=opt_params["muon"]))
        opt.setdefault("head", OptimizerSpec(name="adamw", kwargs=opt_params["adamw"]))
        opt.setdefault("w_x0", OptimizerSpec(name="adamw", kwargs=opt_params["adamw"]))
        opt.setdefault("w_res", OptimizerSpec(name="adamw", kwargs=opt_params["adamw"]))
        opt.setdefault("ve", OptimizerSpec(name="adamw", kwargs=opt_params["adamw"]))

        self._optimizers = opt
    
    def optimizer_class(self, part: str) -> torch.optim.Optimizer:
        if part not in self._optimizers:
            raise ValueError(f"No optimizer specified for part '{part}'. Available parts: {list(self._optimizers.keys())}.")
        return self._optimizers[part].optimizer_class()
    

class GPTConfig(BaseModel):
    """
    # GPTConfig
    GPTConfig is the configuration class for GPT models. It encapsulates all the necessary settings for
    defining the architecture, tokenizer, and training objectives of a GPT model. It provides methods 
    to save and load configurations. It derives from Pydantic's BaseModel for easy serialization and validation.

    Args:
        name (str): The name of the model.
        tokenizer (TokenizerConfig): Configuration for the tokenizer.
        dir (str | Path): Directory to save/load the model.
        model (TransformerConfig): Configuration for the transformer model.
        loss (LossConfig): Configuration for the training loss.

    ## Methods:
        to_file(mode="json" | "pickle"): Save the configuration to a file in the specified format.
        from_file(model_name: str, model_dir: str | Path): Load the
    """
    model_config = ConfigDict(
        json_encoders={Path: str},
        # frozen=True
    )
    name: str = "ic1" # TODO: change it to something more general like 'base_model' -> generate different config to different state model (pretrained, finetuned, etc.)
    tokenizer: TokenizerConfig = Field(default_factory=TokenizerConfig)
    dirname: str | Path = MODELS_FOLDER
    model: TransformerConfig = Field(default_factory=TransformerConfig)
    loss: LossConfig = Field(default_factory=LossConfig)
    trainer: TrainingConfig = Field(default_factory=TrainingConfig)
    dtype: Dtypes = "bfloat16"
    device: Devices = DEVICE

    def model_post_init(self, context: Any) -> None:
        if isinstance(self.dirname, str):
            self.dirname = Path(self.dirname)
        self.dirname = self.dirname / self.name
        if not self.dirname.exists():
            self.dirname.mkdir(parents=True, exist_ok=True)

        if not hasattr(self.model, "vocab_size"):
            raise ValueError("Model configuration must have a vocab_size attribute.")
        if not hasattr(self.tokenizer, "vocab_size"):
            raise ValueError("Tokenizer configuration must have a vocab_size attribute.")
        if hasattr(self.model, "vocab_size") and hasattr(self.tokenizer, "vocab_size"):
            if self.model.vocab_size != self.tokenizer.vocab_size:
                raise ValueError(f"Model vocab_size ({self.model.vocab_size}) does not match tokenizer vocab_size ({self.tokenizer.vocab_size})")
            
        if not hasattr(self.model, "max_context"):
            raise ValueError("Model configuration must have a max_context attribute.")
        if not hasattr(self.tokenizer, "max_context"):
            raise ValueError("Tokenizer configuration must have a max_context attribute.")
        if hasattr(self.model, "max_context") and hasattr(self.tokenizer, "max_context"):
            if self.model.max_context != self.tokenizer.max_context:
                raise ValueError(f"Model max_context ({self.model.max_context}) does not match tokenizer max_context ({self.tokenizer.max_context})")
            
        self.dtype = getattr(torch, self.dtype)
        self.device = torch.device(self.device)

    def __eq__(self, other: "GPTConfig") -> bool:
        if not isinstance(other, GPTConfig):
            return False
        return self.__dict__ == other.__dict__

    def to_file(self, mode="json") -> None:
        suffix_ = "pickle" if mode == "pickle" else "json"
        if isinstance(self.dirname, str):
            self.dirname = Path(self.dirname)
        path = self.dirname  / f"config.{suffix_}"
        if mode not in ["json", "python", "pickle"]:
            raise ValueError(f"Unsupported mode: {mode}")
        
        with open(str(path), "wb") as f:
            if mode == "pickle":
                pickle.dump(self, f)
            else:
                json.dump(self.model_dump(mode=mode), f, indent=4)
        # self.dirname = Path(self.dirname)

    @classmethod
    def from_file(cls, model_name: str, model_dir: str | Path = MODELS_FOLDER) -> "GPTConfig":
        if isinstance(model_dir, str):
            model_dir = Path(model_dir)
        config_path_json = model_dir / model_name / "config.json"
        config_path_pickle = model_dir / model_name / "config.pickle"
        if config_path_json.exists():
            with open(config_path_json, "r", encoding="utf-8") as f:
                config_dict = json.load(f)
            return cls.model_validate(config_dict)
        elif config_path_pickle.exists():
            with open(config_path_pickle, "rb") as f:
                config: GPTConfig = pickle.load(f)
            return config
        else:
            raise FileNotFoundError(f"No configuration file found for model {model_name} in {model_dir}")
        
    @classmethod
    def from_yaml(cls, path: str | Path) -> "GPTConfig":
        import yaml
        if isinstance(path, str):
            path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"No such file: {path}")
        with open(path, "r", encoding="utf-8") as f:
            config_dict = yaml.safe_load(f)
        return cls.model_validate(config_dict)
    
class TransformerOutput(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    logits: torch.Tensor
    attentions: List[torch.Tensor] | None = None
    hidden_states: List[torch.Tensor] | None = None
    past_key_values: Any | dict | None = None

class ModelOutput(TransformerOutput):
    loss: Optional[torch.Tensor] = None
    log_probs: Optional[torch.Tensor] = None

class ModelCompletionOutput(ModelOutput):
    completions: Optional[List[str]] = None
    done: bool = False

class TrainingState(BaseModel):
    step: int = 0
    best_val_loss: float = float("inf")
    early_stopping_counter: int = 0
    train_losses: List[float] = Field(default_factory=list)
    val_losses: List[float] = Field(default_factory=list)

class TrainingResults(BaseModel):
    train_loss: List[float] = Field(default_factory=list)
    val_loss: List[float] = Field(default_factory=list)
    steps: List[int] = Field(default_factory=list)

class TrainingMetrics(BaseModel):
    time: List[float] = Field(default_factory=list)
    step: List[int] = Field(default_factory=list)
    tokens: List[int] = Field(default_factory=list)
    epochs: List[int] = Field(default_factory=list)
    accuracy: List[float] = Field(default_factory=list)
    loss: List[float] = Field(default_factory=list)
    val_accuracy: List[float] = Field(default_factory=list)
    val_loss: List[float] = Field(default_factory=list)
    best_val_loss: List[float] = Field(default_factory=list)
    core: List[float] = Field(default_factory=list)

def get_config_from_huggingface(model_name: str) -> TransformerConfig:
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    vocab_size = tokenizer.vocab_size
    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    return TransformerConfig(
        tokenizer=tokenizer.encode,
        pad_id=pad_id,
        vocab_size=vocab_size
    )